{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "import math"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The cell below loads all data. Make sure your filepaths are updated to work on your computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.read_csv('reddit_text_sentiment.csv')\n",
    "stocks_df = pd.read_csv(\"../data/stocks/csv/stock_prices.csv\")\n",
    "companies_df = pd.read_csv(\"../data/stocks/csv/companies.csv\")\n",
    "brands_df = pd.read_csv(\"../data/stocks/csv/brands.csv\")\n",
    "industries_df = pd.read_csv(\"../data/stocks/csv/industries.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Below three cells add the 'datetime' column to stocks_df and reddit_df. This column contains datetime format object of stock market closing time (EST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to convert utc to EST date\n",
    "eastern = timezone('US/Eastern')\n",
    "def utc_to_est(utc):\n",
    "    return datetime.fromtimestamp(utc, tz = eastern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new stocks column with datetime format of daily market close times\n",
    "stocks_df[\"date\"] = pd.to_datetime(stocks_df[\"date\"]).values.astype(np.int64) // 10**6\n",
    "stocks_df[\"date\"] = (stocks_df[\"date\"] + 57600000)//1000\n",
    "stocks_df['datetime'] = stocks_df['date'].apply(utc_to_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created new reddit column with datetime format of daily market close times\n",
    "reddit_df['datetime'] = reddit_df['created_utc'].apply(utc_to_est)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Following cell does some basic manipulation for the reddit_df:\n",
    "1. select the columns that I will be using (save space and time)\n",
    "2. add tally to posts so that I can get average weighted sentiment scores later\n",
    "3. convert nonzero scores to something trivial\n",
    "4. create weighted_scores column (scores weighted as reddit_score*sentiment_score)\n",
    "5. add column for company_id by merging with brands_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select relevant columns\n",
    "reddit_df = reddit_df[['subreddit', 'datetime', 'score', 'compound', 'positive', 'neutral', 'negative']]\n",
    "\n",
    "#add column that helps in counting posts when grouped\n",
    "reddit_df['num_posts'] = 1\n",
    "\n",
    "#replace zeros with insubstatial float in reddit scores\n",
    "reddit_df.score = reddit_df.score.apply(lambda x: max(x, 0.01))\n",
    "\n",
    "#weighted scores\n",
    "for s in ['compound', 'negative', 'positive', 'neutral']:\n",
    "    reddit_df['weighted_{0}'.format(s)] = reddit_df[s]*reddit_df['score']\n",
    "    \n",
    "#add company_id as column\n",
    "reddit_df = reddit_df.merge(brands_df[['subreddit', 'company_id']], on='subreddit')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Following cells are helper functions:\n",
    "\n",
    "1. separate_reddit takes in the full reddit dataframe and outputs a list of sub-dataframes that are separated either by company, industry, or not at all. The 'by' argument must be one of 'company', 'industry', or 'all'.\n",
    "\n",
    "2. group_data function is meant to be applied to every dataframe output in the separate_reddit output list. This function takes df, by, and days as arguments. 'df' should be a dataframe obtained from the separate_reddit function. 'by' should be one of the strings allowed by the separate_reddit function 'by' argument. 'days' should be an integer. The function outputs a dataframe with datetime (on the later end of each group), weighted sentiment scores (summed over the rolling window), and the company_id or industry_id (if by=='industry')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function that takes a dataframe and returns separate dataframes for each company\n",
    "def separate_reddit(reddit_df, by='all'):\n",
    "    if by not in ['all', 'company', 'industry']:\n",
    "        print('argument invalid: must be = <all>, <company>, or <industry>')\n",
    "        pass\n",
    "    else:\n",
    "        if by == 'industry':\n",
    "            temp = reddit_df.merge(companies_df[['id', 'industry_id']], left_on = 'company_id', right_on='id')\n",
    "            return [temp[temp['industry_id']==i][['industry_id', 'datetime','weighted_compound', 'weighted_negative', 'weighted_positive', 'weighted_neutral', 'num_posts']] for i in temp.industry_id.unique()]\n",
    "        elif by == 'company':\n",
    "            return [reddit_df[reddit_df['company_id']==i][['company_id', 'datetime','weighted_compound', 'weighted_negative', 'weighted_positive', 'weighted_neutral', 'num_posts']] for i in reddit_df.company_id.unique()]\n",
    "        else:\n",
    "            return [reddit_df[['company_id', 'datetime','weighted_compound', 'weighted_negative', 'weighted_positive', 'weighted_neutral', 'num_posts']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groups data for a single company using sliding window - number of days specified in call\n",
    "def group_data(df, by='all', days = 1):\n",
    "    if by not in ['all', 'company', 'industry']:\n",
    "        print('invalid arg: must be in [all, company, industry]')\n",
    "        pass\n",
    "    else:\n",
    "        if by=='industry':\n",
    "            i_id = df.industry_id.unique()[0] #save industry_id\n",
    "            temp_df = df[['datetime', 'weighted_compound', 'weighted_positive', 'weighted_neutral', 'weighted_negative', 'num_posts']].groupby(pd.Grouper(key='datetime', freq='24h', base=11, label='right')).sum() #groupby day\n",
    "            min_date = min(temp_df.index)\n",
    "            max_date = max(temp_df.index)\n",
    "            date_idx = [i for i in pd.date_range(min_date, max_date)] #new index\n",
    "            temp_df = temp_df.reindex(date_idx).fillna(0).rolling(days).sum()[days-1:] #make dataframe with rolling window sum\n",
    "            temp_df['industry_id'] = i_id #restore industry_id\n",
    "            temp_df.reset_index(inplace=True)\n",
    "            return temp_df\n",
    "        elif by=='company':\n",
    "            c_id = df.company_id.unique()[0] #save company_id\n",
    "            temp_df = df[['datetime', 'weighted_compound', 'weighted_positive', 'weighted_neutral', 'weighted_negative', 'num_posts']].groupby(pd.Grouper(key='datetime', freq='24h', base=11, label='right')).sum() #groupby day\n",
    "            min_date = min(temp_df.index)\n",
    "            max_date = max(temp_df.index)\n",
    "            date_idx = [i for i in pd.date_range(min_date, max_date)] #new index\n",
    "            temp_df = temp_df.reindex(date_idx).fillna(0).rolling(days).sum()[days-1:] #make dataframe with rolling window sum\n",
    "            temp_df['company_id'] = c_id #restore company_id\n",
    "            temp_df.reset_index(inplace=True)\n",
    "            return temp_df\n",
    "        else:\n",
    "            return pd.concat([group_data(d, 'company', days) for d in separate_reddit(df, 'company')])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Follwing cell makes all of the dataframes ready for regression. Separates a given datafram, runs the group_data function on every one of them, calculates average weighted sentiment scores, and adds change percent to each dataframe. Outpust a list of dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframes(df, separation='all', lookback=1):\n",
    "    dfs = []\n",
    "    separated = separate_reddit(df, separation)\n",
    "    for d in separated:\n",
    "        temp = group_data(d, separation, lookback)\n",
    "        for i in ['weighted_compound', 'weighted_negative', 'weighted_positive', 'weighted_neutral']: #iterate over scores columns\n",
    "            temp['avg_{}'.format(i)] = temp[i]/temp['num_posts'].apply(lambda x: max(x, 1)) #set avgerage weighted scores columns\n",
    "        temp = temp.merge(stocks_df[['company_id', 'datetime', 'change_percent']], on=['company_id', 'datetime']) #add change_percent column\n",
    "        dfs.append(temp)\n",
    "    return dfs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
