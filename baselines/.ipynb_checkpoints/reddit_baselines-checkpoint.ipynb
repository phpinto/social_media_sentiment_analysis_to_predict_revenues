{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The cell below loads all data. Make sure your filepaths are updated to work on your computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.read_csv('reddit_text_sentiment.csv')\n",
    "stocks_df = pd.read_csv(\"../data/stocks/csv/stock_prices.csv\")\n",
    "companies_df = pd.read_csv(\"../data/stocks/csv/companies.csv\")\n",
    "brands_df = pd.read_csv(\"../data/stocks/csv/brands.csv\")\n",
    "industries_df = pd.read_csv(\"../data/stocks/csv/industries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_df = pd.read_csv(\"../data/stocks/csv/stock_prices.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Below three cells add the 'datetime' column to stocks_df and reddit_df. This column contains datetime format object of stock market closing time (EST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to convert utc to EST date\n",
    "eastern = timezone('US/Eastern')\n",
    "def utc_to_est(utc):\n",
    "    return datetime.fromtimestamp(utc, tz = eastern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new stocks column with datetime format of daily market close times\n",
    "stocks_df[\"date\"] = pd.to_datetime(stocks_df[\"date\"]).values.astype(np.int64) // 10**6\n",
    "stocks_df[\"date\"] = (stocks_df[\"date\"] + 57600000)//1000\n",
    "stocks_df['datetime'] = stocks_df['date'].apply(utc_to_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created new reddit column with datetime format of daily market close times\n",
    "reddit_df['datetime'] = reddit_df['created_utc'].apply(utc_to_est)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Following cell does some basic manipulation for the reddit_df:\n",
    "1. select the columns that I will be using (save space and time)\n",
    "2. add tally to posts so that I can get average weighted sentiment scores later\n",
    "3. convert nonzero scores to something trivial\n",
    "4. create weighted_scores column\n",
    "5. add column for company_id by merging with brands_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select relevant columns\n",
    "reddit_df = reddit_df[['subreddit', 'datetime', 'score', 'compound', 'positive', 'neutral', 'negative']]\n",
    "\n",
    "#add column that helps in counting posts when grouped\n",
    "reddit_df['num_posts'] = 1\n",
    "\n",
    "#replace zeros with insubstatial float in reddit scores\n",
    "reddit_df.score = reddit_df.score.apply(lambda x: max(x, 0.01))\n",
    "\n",
    "#weighted scores\n",
    "for s in ['compound', 'negative', 'positive', 'neutral']:\n",
    "    reddit_df['weighted_{0}'.format(s)] = reddit_df[s]*reddit_df['score'].apply(lambda x: math.log(x+1))\n",
    "    \n",
    "#add company_id as column\n",
    "reddit_df = reddit_df.merge(brands_df[['subreddit', 'company_id']], on='subreddit')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Following cells are helper functions:\n",
    "\n",
    "1. separate_reddit takes in the full reddit dataframe and outputs a list of sub-dataframes that are separated either by company, industry, or not at all. The 'by' argument must be one of 'company', 'industry', or 'all'.\n",
    "\n",
    "2. group_data function is meant to be applied to every dataframe output in the separate_reddit output list. This function takes df, by, and days as arguments. 'df' should be a dataframe obtained from the separate_reddit function. 'by' should be one of the strings allowed by the separate_reddit function 'by' argument. 'days' should be an integer. The function outputs a dataframe with datetime (on the later end of each group), weighted sentiment scores (summed over the rolling window), and the company_id or industry_id (if by=='industry')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function that takes a dataframe and returns separate dataframes for each company\n",
    "def separate_reddit(reddit_df, by='all'):\n",
    "    if by not in ['all', 'company', 'industry']:\n",
    "        print('argument invalid: must be = <all>, <company>, or <industry>')\n",
    "        pass\n",
    "    else:\n",
    "        if by == 'industry':\n",
    "            temp = reddit_df.merge(companies_df[['id', 'industry_id']], left_on = 'company_id', right_on='id')\n",
    "            return [temp[temp['industry_id']==i][['industry_id', 'datetime','weighted_compound', 'weighted_negative', 'weighted_positive', 'weighted_neutral', 'num_posts']] for i in temp.industry_id.unique()]\n",
    "        elif by == 'company':\n",
    "            return [reddit_df[reddit_df['company_id']==i][['company_id', 'datetime','weighted_compound', 'weighted_negative', 'weighted_positive', 'weighted_neutral', 'num_posts']] for i in reddit_df.company_id.unique()]\n",
    "        else:\n",
    "            return [reddit_df[['company_id', 'datetime','weighted_compound', 'weighted_negative', 'weighted_positive', 'weighted_neutral', 'num_posts']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groups data for a single company using sliding window - number of days specified in call\n",
    "def group_data(df, by='all', days = 1):\n",
    "    if by not in ['all', 'company', 'industry']:\n",
    "        print('invalid arg: must be in [all, company, industry]')\n",
    "        pass\n",
    "    else:\n",
    "        if by=='industry':\n",
    "            i_id = df.industry_id.unique()[0] #save industry_id\n",
    "            temp_df = df[['datetime', 'weighted_compound', 'weighted_positive', 'weighted_neutral', 'weighted_negative', 'num_posts']].groupby(pd.Grouper(key='datetime', freq='24h', base=11, label='right')).sum() #groupby day\n",
    "            min_date = min(temp_df.index)\n",
    "            max_date = max(temp_df.index)\n",
    "            date_idx = [i for i in pd.date_range(min_date, max_date)] #new index\n",
    "            temp_df = temp_df.reindex(date_idx).fillna(0).rolling(days).sum()[days-1:] #make dataframe with rolling window sum\n",
    "            temp_df['industry_id'] = i_id #restore industry_id\n",
    "            temp_df.reset_index(inplace=True)\n",
    "            return temp_df\n",
    "        elif by=='company':\n",
    "            c_id = df.company_id.unique()[0] #save company_id\n",
    "            temp_df = df[['datetime', 'weighted_compound', 'weighted_positive', 'weighted_neutral', 'weighted_negative', 'num_posts']].groupby(pd.Grouper(key='datetime', freq='24h', base=11, label='right')).sum() #groupby day\n",
    "            min_date = min(temp_df.index)\n",
    "            max_date = max(temp_df.index)\n",
    "            date_idx = [i for i in pd.date_range(min_date, max_date)] #new index\n",
    "            temp_df = temp_df.reindex(date_idx).fillna(0).rolling(days).sum()[days-1:] #make dataframe with rolling window sum\n",
    "            temp_df['company_id'] = c_id #restore company_id\n",
    "            temp_df.reset_index(inplace=True)\n",
    "            return temp_df\n",
    "        else:\n",
    "            return pd.concat([group_data(d, 'company', days) for d in separate_reddit(df, 'company')])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Following cell makes all of the dataframes ready for regression. Separates a given datafram, runs the group_data function on every one of them, calculates average weighted sentiment scores, and adds change percent to each dataframe. Outpust a list of dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframes(df, separation='all', lookback=1, min_date = None, max_date = None):\n",
    "    if min_date:\n",
    "        df = df[df['datetime'] >= pd.to_datetime(min_date).tz_localize('US/Eastern')]\n",
    "    if max_date:\n",
    "        df = df[df['datetime'] <= pd.to_datetime(max_date).tz_localize('US/Eastern')]\n",
    "    dfs = []\n",
    "    separated = separate_reddit(df, separation)\n",
    "    for d in separated:\n",
    "        temp = group_data(d, separation, lookback)\n",
    "        for i in ['weighted_compound', 'weighted_negative', 'weighted_positive', 'weighted_neutral']: #iterate over scores columns\n",
    "            temp['avg_{}'.format(i)] = temp[i]/temp['num_posts'].apply(lambda x: max(x, 1)) #set avgerage weighted scores columns\n",
    "        temp = temp.merge(stocks_df[['company_id', 'datetime', 'change_percent']], on=['company_id', 'datetime']) #add change_percent column\n",
    "        dfs.append(temp)\n",
    "    return dfs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dfs = make_dataframes(reddit_df, lookback = 1, min_date = '2014-03-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a299e48cf8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(model_dfs[0].avg_weighted_compound, model_dfs[0].change_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Some tests for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function that finds optimal C for a given training set (logistic regression)\n",
    "def cross_validate_logreg(X_train, y_train, c_s = [0.1, 0.3, 1, 3, 10, 30, 100]):\n",
    "    c_s = [0.1, 0.3, 1, 3, 10, 30, 100]\n",
    "    lg_cv = LogisticRegressionCV(Cs = c_s, scoring='f1').fit(X_train, y_train)\n",
    "    return c_s[np.argmax(np.mean(lg_cv.scores_[True],axis=0))], np.max(np.mean(lg_cv.scores_[True], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_by_lookback(reddit_df, lookback = [1], c_s = [0.1, 0.3, 1, 3, 10, 30, 100]):\n",
    "    for i in lookback:\n",
    "        full_df = make_dataframes(reddit_df, lookback = i, min_date = '2014-03-01')[0]\n",
    "        X = full_df[['avg_weighted_compound', 'avg_weighted_positive', 'avg_weighted_negative', 'avg_weighted_neutral', 'num_posts']]\n",
    "        y = full_df['change_percent'] > 0\n",
    "        X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.25, random_state = 6240)\n",
    "        best_c, f1_score = cross_validate_logreg(X_train, y_train, c_s)\n",
    "        print('Best f1_score = {0} for {1} day lookback with C = {2}'.format(f1_score, i, best_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1_score = 0.6844993070286836 for 1 day lookback with C = 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1_score = 0.6845026628116683 for 3 day lookback with C = 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1_score = 0.6507838626984754 for 6 day lookback with C = 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1_score = 0.6825146042805549 for 10 day lookback with C = 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1_score = 0.6855929478481713 for 15 day lookback with C = 30\n"
     ]
    }
   ],
   "source": [
    "logreg_by_lookback(reddit_df, lookback = [1, 3, 6, 10, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method format of str object at 0x000001A297D4D170>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#test out best logistic regression model\n",
    "full_df = make_dataframes(reddit_df, lookback = 15, min_date = '2014-03-01')[0]\n",
    "X = full_df[['avg_weighted_compound', 'avg_weighted_positive', 'avg_weighted_negative', 'avg_weighted_neutral', 'num_posts']]\n",
    "y = full_df['change_percent'] > 0\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.25, random_state = 6240)\n",
    "lg = LogisticRegression(C=30).fit(X_train, y_train)\n",
    "lg_preds = lg.predict(X_test)\n",
    "f1 = f1_score(lg_preds, y_test)\n",
    "print('f1 score :{0}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r-squared for 1 day lookup is 0.0007246047514668508\n",
      "f1 for 1 day lookup is 0.6702033598585323\n",
      "r-squared for 3 day lookup is 0.0005415447738095214\n",
      "f1 for 3 day lookup is 0.6798283261802576\n",
      "r-squared for 6 day lookup is 0.001932734096405375\n",
      "f1 for 6 day lookup is 0.5962666666666666\n",
      "r-squared for 10 day lookup is 0.0007407773187453337\n",
      "f1 for 10 day lookup is 0.6909402441636323\n",
      "r-squared for 15 day lookup is 0.0006469767110658386\n",
      "f1 for 15 day lookup is 0.6789748045178107\n"
     ]
    }
   ],
   "source": [
    "##Baseline tests for linear regression\n",
    "lookback_days = [1, 3, 6, 10, 15]\n",
    "for i in lookback_days:\n",
    "    full_df = make_dataframes(reddit_df, lookback = i, min_date = '2014-03-01')[0]\n",
    "    X = full_df[['avg_weighted_compound', 'avg_weighted_positive', 'avg_weighted_negative', 'avg_weighted_neutral', 'num_posts']]\n",
    "    y = full_df['change_percent']\n",
    "    X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.25, random_state = 6240)\n",
    "    ln = LinearRegression(normalize=True).fit(X_train, y_train)\n",
    "    ln_pred = ln.predict(X_test)\n",
    "    ln_f1 = f1_score((ln_pred > 0), y_test >0)\n",
    "    print('r-squared for {0} day lookup is {1}'.format(i, ln.score(X_train, y_train)))\n",
    "    print('f1 for {0} day lookup is {1}'.format(i, ln_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
